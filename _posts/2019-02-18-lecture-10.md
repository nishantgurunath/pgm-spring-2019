---
layout: distill
title: "Lecture 10: Sequential Models"
description: Introducing State Space Models and Kalman filters.
date: 2019-02-18

lecturers:
  - name: Eric Xing
    url: "https://www.cs.cmu.edu/~epxing/"

authors:
  - name: Abhishek Mohta  # author's full name
    url: "#"  # optional URL to the author's homepage
  - name: Alankar Jain
    url: "#"
  - name: Nishant Gurunath
    url: "#"
  - name: Zirui Wang
    url: "#"
editors:
  - name: Hao Zhang  # editor's full name
    url: "#"  # optional URL to the editor's homepage

abstract: >
  In this lecture, we transition from Gaussian graphical models and discrete/continuous HMMs to State Space models.
  We begin with some review of the past lectures on HMMs, basic matrix algebra and building an intuition for SSM.
  After that we explain factor analysis.
---

## Mixture Models Review and Intuition to State Space Models
<figure id="first-slide-review">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-10/first-slide.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong> Review and intuition of HMMs and SSMs </strong>
  </figcaption>
</figure>
<ol>
  <li> Mixture model has the observation X and their corresponding hidden latent variable Y which indicates the source of that observation X.</li>
	<ol type="a">
		<li> X – can be discrete like word frequency, or continuous – temperature</li>
		<li> Y – which mixture/distribution it comes from</li>
		<li> When both X and Y are continuous – it is called factor analysis</li>
	</ol>
  <li> The advantage with mixture models is that smaller graphs (mixture models) can be taken as building blocks to make bigger graphs (HMMs)</li>
  <li> Some common inference algorithms for HMMs can range from inferring on one hidden state, all hidden states, most probable sequence of hidden states. Example algorithms -</li> 
	<ol type="a">
		<li> Viterbi algorithm</li>
		<li> Forward/Backward algorithm</li>
		<li> Baum-Welch algorithm</li>
	</ol>
  <li> All the above algorithms have a counterpart in State Space models - despite the mathematical technique being different, they have the same essence. A similarity to the above algorithms that it shares is that it can be broken down into local operations/subproblems and continuously built to the whole solution. We will further explain in the coming sections.</li>
  <li> Stories/Intuitions for the various models</li>
	<ol type="a">
		<li> HMM - Dishonest casino story</li>
		<li> Factorial HMM - Multiple dealers at the dishonest casino</li>
		<li> State Space Model (SSM) - X – Signal of an aircraft on the radar, Y – actual physical locations of the aircraft</li>
		<li> Switching SSM - multiple aircrafts (State S is the indicator of which aircraft is appearing on the radar)</li>
	</ol>
</ol> 

## Basic Math Review
<ol>
	<li>A multivariate Gaussian is denoted by the following PDF -  </li>
 
<d-math block center>
P(X | \mu, \Sigma) = \frac{1}{(2\pi)^{n/2} |\Sigma|^{1/2}} exp(\frac{-1}{2}(X - \mu)^{T}\Sigma^{-1}(X-\mu))
</d-math>

	<li>Joint Gaussian is denoted by - </li>

<d-math block center>

P(\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} | \mu, \Sigma) = \mathcal{N}(\begin{bmatrix} X_1 \\ X_2 \end{bmatrix} | \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix}, \begin{bmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{bmatrix})
</d-math>
	<li> Given the joint distribution, we can write - </li>
<d-math block center>
\begin{aligned}
P(X_2) = \mathcal{N}(X_2 | \mu_2, \Sigma_{22}) \\
P(X_1 | X_2) = \mathcal{N}(X_1 | \mu_{1|2}, V_{1|2}) \\
\mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(X_2 - \mu_2) \\
V_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} 
\end{aligned}
</d-math>	

	<li> Matrix inverse lemma - </li>
	<ol>
		<li>Consider a block partitioned matrix M = 
		<d-math block>
			\begin{bmatrix} E & F \\ G & H \end{bmatrix}
		</d-math>
		</li>
		<li>First we diagonalize M  -
		<d-math block>
                        \begin{bmatrix} I & -FH^{-1} \\ 0 & I \end{bmatrix}\begin{bmatrix} E & F \\ G & H \end{bmatrix}\begin{bmatrix} I & 0 \\ -H^{-1}G & I \end{bmatrix} = \begin{bmatrix} E-FH^{-1}G & 0 \\ 0 & H \end{bmatrix}
                </d-math>
		</li>
		<li>This is called the Schur's complement - 
		<d-math block>
			M/H = E-FH^{-1}G
		</d-math>
		</li>
		<li>Then we inverse using the formula -
		<d-math block>
			\begin{aligned}
			XYZ = W \implies Y^{-1} = ZW^{-1}X
			\implies M^{-1} = \begin{bmatrix} E & F \\ G & H \end{bmatrix}^{-1} = \begin{bmatrix} I & 0 \\ -H^{-1}G & I \end{bmatrix}\begin{bmatrix} (M/H)^{-1} & 0 \\ 0 & H^{-1} \end{bmatrix}\begin{bmatrix} I & -FH^{-1} \\ 0 & I \end{bmatrix} \\
			= \begin{bmatrix} (M/H)^{-1} & -(M/H)^{-1}FH^{-1} \\ -H^{-1}G(M/H)^{-1} & H^{-1} + H^{-1}G(M/H)^{-1}FH^{-1} \end{bmatrix} \\
			= \begin{bmatrix} E^{-1} + E^{-1}F(M/E)^{-1}GE^{-1} & -E^{-1}F(M/E)^{-1} \\ -(M/E)^{-1}GE^{-1} & (M/E)^{-1} \end{bmatrix}
			\end{aligned}
		
		</d-math>
		</li>
		<li>Hence, by matrix inverse lemma, 
		<d-math block>
			(E - FH^{-1}G)^{-1} = E^{-1}+ E^{-1}F(H-GE^{-1}F)^{-1}GE^{-1}
		</d-math>
		</li>
	</ol>
	<li> Basic matrix algebra - 
	<ol>
		<li> Trace - 
		<d-math block>
			tr[A]^{def} = \Sigma_i a_{ii}
		</d-math>
		</li>
		<li> tr[ABC] = tr[CBA] = tr[BCA] </li>
		<li> Derivatives -  
		<d-math block>
		\begin{aligned}
			\frac{\partial}{\partial A}tr[BA] = B^{T} \\
			\frac{\partial}{\partial A}tr[x^{T}Ax] = \frac{\partial}{\partial A}tr[xx^{T}A] = xx^{T}
		\end{aligned}
		</d-math>
		</li>
		<li> Derivates - 
		<d-math block>
                        \frac{\partial}{\partial A}log|A| = A^{-1}
                </d-math>
		</li>
	</ol>
	</li>
</ol>

## Factor analysis
Imagine a point on a sheet of paper <d-math block>x \in \mathcal{R}^2</d-math>
If the orientation or the axes are changed, it could be viewed as a point in 3D-space as well. Depending on whether one is sitting in the room or on the paper, we will see the point in 2D or 3D. Hence Y is what is observed from our view (room or on paper), and X (the original point) is what is the hidden/latent variable.

<d-math block center>
\begin{aligned}
P(X) = \mathcal{N}(X;0, I) \\
P(Y | X) = \mathcal{N}(Y; \mu+\Lambda X, \Psi)
\end{aligned}
</d-math>

<figure id="fa-intro">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-10/point.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong> Factor analysis for a point on a paper </strong>
  </figcaption>
</figure>

We know that, a marginal gaussian (p(X)) times a conditional gaussian (p(Y|X)) is a joint gaussian, and a marginal (p(Y)) of a joint gaussian (p(X, Y)) is also a gaussian. Hence, we can compute the mean and variance of Y.
Assuming noise W is uncorrelated with the data, 
<d-math block>
\begin{aligned}
	W = \mathcal{N}(0, \Psi) \\
	E[Y] = E[\mu + \Lambda X + W] \\
	     = \mu + 0 + 0 = \mu \\
	Var[Y] = E[(Y-\mu)(Y-\mu)^T] \\
		= E[(\mu + \Lambda X + w - \mu)(\mu + \Lambda X + w - \mu)^T] \\
		= E[(\Lambda X + w)(\Lambda X + w)^T] \\
		= \Lambda E[XX^T]\Lambda^T + E[WW^T] \\
		= \Lambda \Lambda^T + \Psi
\end{aligned} 
</d-math> 

Hence, the marginal density for factor analysis (Y is p-dim, X is k-dim):
<d-math block>
	P(Y|\theta)= \mathcal{N}(Y;\mu, \Lambda \Lambda^T + \Psi)
</d-math>

We can also say, that the effective covariance matrix is a low-rank outer product of two long skinny matrices plus a diagonal matrix. In other words, factor analysis is just a constrained Gaussian model. 

<figure id="covariance">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-10/covariance.png' | relative_url }}" />
  </div>
  <figcaption>
    <strong> Covariance of the marginal of FA </strong>
  </figcaption>
</figure>

We will now analyse the Factor analysis joint distribution assuming noise is uncorrelated with the data or the latent variables -
<ol>
	<li> The distributions - 
	<d-math block center>
		\begin{aligned}
			P(X) = \mathcal{N}(X;0, I) \\
			P(Y | X) = \mathcal{N}(Y; \mu+\Lambda X, \Psi)
		\end{aligned}
	</d-math>
	</li>
	<li> Covariance between X and Y - 
	<d-math block center>
		\begin{aligned}
			Cov[X, Y] = E[(X - 0)(Y - \mu)^T] = E[X(\mu + \Lambda X + W -\mu)^T] \\
				= E[XX^T\Lambda^T + XW^T] = \Lambda^T
		\end{aligned}
	</d-math>
	</li>
	<li> Hence the joint distribution of X and Y is - 
	<d-math block center>

	P(\begin{bmatrix} X \\ Y \end{bmatrix}) = \mathcal{N}(\begin{bmatrix} X \\ Y \end{bmatrix} | \begin{bmatrix} 0 \\ \mu \end{bmatrix}, \begin{bmatrix} I & \Lambda^T \\ \Lambda & \Lambda \Lambda^T + \Psi \end{bmatrix})
</d-math>
	</li>
	<li> Now we can say - 
	<d-math block center>
	\begin{aligned}
		\Sigma_{11} = I \\
		\Sigma_{12} = \Sigma_{21}^T = \Lambda^T \\
		\Sigma_{22} = \Lambda \Lambda^T + \Psi
	\end{aligned}
	</d-math>
	</li>
	<li>We can now derive the posterior of the latent variable X given Y, where
	<d-math block>
	\begin{aligned}
		P(X|Y) = \mathcal{N}(X | \mu_{1|2}, V_{1|2}) \\
		\mu_{1|2} = \mu_1 + \Sigma_{12}\Sigma_{22}^{-1}(X_2 - \mu_2) \\
			= \Lambda^T(\Lambda\Lambda^T + \Psi)^{-1}(Y - \mu) \\
		V_{1|2} = \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
			= I - I\Lambda^T(\Lambda\Lambda^T + \Psi)^{-1}\Lambda I
	\end{aligned}
	</d-math>
	</li>
	<li> Applying the matrix inversion lemma we learnt above, we get - 
	<d-math block>
	\begin{aligned}
		(E - FH^{-1}G)^{-1} = E^{-1}+ E^{-1}F(H-GE^{-1}F)^{-1}GE^{-1} \\
		V_{1|2} = (I + \Lambda^T\Psi^{-1}\Lambda)^{-1} \\
		\mu_{1|2} = V_{1|2}\Lambda^T\Psi^{-1}(Y-\mu)
	\end{aligned}
	</d-math>
	</li>
</ol>


## Kalman Filter

The Kalman Filter is an algorithm analogous to the forward algorithm for HMM. For the state space model (SSM), the goal of Kalman filter is to estimate the belief state $P(X_{t}\|y_{1:t})$ given the data $\{y_{1},...,y_{t}\}$. To do so, it mainly follows a recursive procedure that includes two steps, namely time update and measurement update. Here, we are going to derive the two steps.

#### Derivation

+ **Time Update:** 

(1) Goal: Compute $P(X_{t+1}\|y_{1:t})$ (the distribution for $X_{t+1\|t}$) using prior belief $P(X_{t}\|y_{1:t})$ and the dynamical model $P(X_{t+1}\|X_{t})$.

(2) Derivation: Recall that the dynamical model states that $$X_{t+1}=AX_{t}+Gw_{t}$$ where $w_{t}$ is the noise term with a Gaussian distribution $\mathcal{N}(0; Q)$. Then, we can conpute the mean and variance for $X_{t+1}$ using this formula. 

For the mean, we have:
<d-math block>
	\begin{aligned}
		\hat{X}_{t+1|t} & = \mathbb{E}(X_{t+1}|y_{1},...,y_{t})\\
		& = \mathbb{E}(AX_{t}+Gw_{t})\\
		& = A \hat{X}_{t|t}
	\end{aligned}
</d-math>

For the variance, we have:
<d-math block>
	\begin{aligned}
		P_{t+1|t} & = \mathbb{E}((X_{t+1}-\hat{X}_{t+1|t})(X_{t+1}-\hat{X}_{t+1|t})^T|y_{1},...,y_{t})\\
		& = \mathbb{E}((AX_{t}+Gw_{t}-\hat{X}_{t+1|t})(AX_{t}+Gw_{t}-\hat{X}_{t+1|t})^T|y_{1},...,y_{t})\\
		& = AP_{t|t}A^T+GQG^T
	\end{aligned}
</d-math>

+ **Measurement Update:** 

(1) Goal: Compute $P(X_{t+1}\|y_{1:t+1})$ using $P(X_{t+1}\|y_{1:t})$ computed from time update, observation $y_{t+1}$ and observation model $P(y_{t+1}\|X_{t+1})$.

(2) Derivation: The key here is to first compute the joint distribution $P(X_{t+1},y_{t+1}\|y_{1:t})$, then derive the conditional distribution $P(X_{t+1}\|y_{1:t+1})$. Recall that the observation model states that $$y_{t}=CX_{t}+v_{t}$$ where $v_{t}$ is the noise term with a Gaussian distribution $\mathcal{N}(0; R)$. We have already computed the mean and the variance of $X_{t+1\|t}$ in the time update. Now we need to compute the mean and the variance of $y_{t+1\|t}$ and the covariance of $X_{t+1\|t}$ and $y_{t+1\|t}$.

For the mean of $y_{t+1\|t}$, we have:
<d-math block>
	\begin{aligned}
		\hat{y}_{t+1|t} & = \mathbb{E}(y_{t+1}|y_{1},...,y_{t})\\
		& = \mathbb{E}(CX_{t+1}+v_{t+1})\\
		& = C \hat{X}_{t+1|t}
	\end{aligned}
</d-math>

For the varaince of $y_{t+1\|t}$, we have:
<d-math block>
	\begin{aligned}
		\mathbb{E}((y_{t+1}-\hat{y}_{t+1|t})(y_{t+1}-\hat{y}_{t+1|t})^T|y_{1},...,y_{t}) & = \mathbb{E}((y_{t+1}-C \hat{X}_{t+1|t})(y_{t+1}-C \hat{X}_{t+1|t})^T|y_{1},...,y_{t})\\
		& = C P_{t+1|t} C^T + R
	\end{aligned}
</d-math>


For the covaraince of $X_{t+1\|t}$ and $y_{t+1\|t}$, we have:
<d-math block>
	\begin{aligned}
		\mathbb{E}((y_{t+1}-\hat{y}_{t+1|t})(X_{t+1}-\hat{X}_{t+1|t})^T|y_{1},...,y_{t}) = C P_{t+1|t}
	\end{aligned}
</d-math>

Hence the joint distribution of $X_{t+1\|t}$ and $y_{t+1\|t}$ is:
<d-math block center>
\mathcal{N}(\begin{bmatrix} X_{t+1|t} \\ y_{t+1|t} \end{bmatrix} | \begin{bmatrix} \hat{X}_{t+1|t} \\ C\hat{X}_{t+1|t} \end{bmatrix}, \begin{bmatrix} P_{t+1|t} & P_{t+1|t}C^T \\ CP_{t+1|t} & C P_{t+1|t} C^T + R \end{bmatrix})
</d-math>

Finally, to compute the conditional distribution of a joint Gaussian distribution, we recall the formulas as:
<d-math block>
	\begin{aligned}
		m_{1|2} & = \mu_1+\Sigma_{12}\Sigma_{22}^{-1}(x_2-\mu_2) \\
		V_{1|2} & = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21}
	\end{aligned}
</d-math>
Plugging in results from above we have the conditional distribution of $P(X_{t+1}\|y_{1:t+1})$ as:
<d-math block>
	\begin{aligned}
		\hat{X}_{t+1|t+1} & = \hat{X}_{t+1|t} + K_{t+1}(y_{t+1}-C\hat{X}_{t+1|t}) \\
		P_{t+1|t+1} & = P_{t+1|t} - K_{t+1} C P_{t+1|t}
	\end{aligned}
</d-math>
where $K_{t+1}  = P_{t+1\|t}C^T(CP_{t+1\|t}C^T+R)^{-1}$ is reffered to as the Kalman gain matrix.

#### Example

We now see an example of Kalman filter on a problem on noisy observations of a 1D particle doing a random walk. The SSM is given as:
<d-math block>
	\begin{aligned}
		X_{t+1|t} & = X_{t} + w \\
		y_{t} & = X_{t} + v
	\end{aligned}
</d-math>
where $w \sim \mathcal{N}(0; \sigma_{x})$ and $v \sim \mathcal{N}(0; \sigma_{y})$ are noises. In other words, $A=G=C=I$. Thus, using the update rules derived above we have that at time t:
<d-math block>
	\begin{aligned}
		P_{t+1|t} & = \sigma_{t}+\sigma_{x} \\
		\hat{X}_{t+1|t} & = X_{t|t} \\
		K_{t+1} & = (\sigma_t+\sigma_x)(\sigma_t+\sigma_x+\sigma_y)\\
		\hat{X}_{t+1|t+1} & = ((\sigma_t+\sigma_x)y_t+\sigma_y\hat{X}_{t|t})/(\sigma_t+\sigma_x+\sigma_y)\\
		P_{t+1|t+1} & = ((\sigma_t+\sigma_x)\sigma_y)/(\sigma_t+\sigma_x+\sigma_y)
	\end{aligned}
</d-math>

As demonstrated in the figure below, given prior belief $P(X_0)$, our estimate of $P(X_1)$ without an observation has the same mean and larger variance as shown by the first two lines in equations above. However, given an observation of $2.5$, the new estimated distribution shifted to the right accoding to last three lines of equations above.

<figure id="first-slide-review">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-10/kalman_filter_example.png' | relative_url }}" style="width: 50%; height: 50%" />
  </div>
  <figcaption>
    <strong> 1D Kalman Filter example </strong>
  </figcaption>
</figure>

+ **Complexity of one KF step:**

Let $X_t \in R^{N_x}$ and $Y_t \in R^{N_y}$

Predict step:
$P_{t+1|t} = AP_{t|t}A + GQG^T$

This invloves matrix multiplication of $N_x$ x $N_x$ matrix with another $N_x$ x $N_x$ matrix, hence the time complexity is $O(N_{x}^{2})$

Measurement Updates:
$K_{t+1} = P_{t+1|t}C^T(CP_{t+1|t}C^T + R)^{-1}$

This invloves a matrix inversion of $N_y$ x $N_y$, hence the time complexity is $O(N_{y}^{3})$

Overall time = max{$N_{x}^{2},N_{y}^{3}$}





**Rauch-Tung-Strievel**

The Rauch-Tung-Strievel algorithm is a Gaussian analog of the forwards-backwards(alpha-gamma) algorithm for HMM. The intent is to estimate $P(X_t\|y_{1:T})$. 


<figure id="first-slide-review">
  <div class="row">
    <img src="{{ '/assets/img/notes/lecture-10/rauch-tung-strievel.png' | relative_url }}" style="width: 50%; height: 50%" />
  </div>
  <figcaption>
    <strong> Rauch-Tung-Strievel </strong>
  </figcaption>
</figure>



Since, $P(X_t\|y_{1:T})$ is a Gaussian distribution, we intend to derive mean $X_{t\|T}$ and variance $P_{t\|T}$.


Let's first define the joint distribution $P(X_{t},X_{t+1}\|y_{1:t})$.

<d-math block>  
m = \begin{bmatrix} \hat{x}_{t|t} \\ \hat{x}_{t+1|t} \end{bmatrix}, V = \begin{bmatrix} P_{t|t} & P_{t|t}A^T \\ AP_{t|t} & P_{t+1|t} \end{bmatrix}
</d-math>
	
We are going to use a neat trick to arrive at the RTS inference.

$E[X\|Z] = E[E[X\|Y,Z]\|Z]$ 

$Var[X\|Z] = Var[E[X\|Y,Z]\|Z] + E[Var[X\|Y,Z]\|Z]$

Now, applying these results to our problem:

<d-math block>
\hat{x}_{t|T} = E[X_t|Y_{1:T}] = E[E[X_t|X_{t+1},Y_{1:T}]| Y_{1:T}] 
</d-math>

X_t is independent of $X_{t+2:T}$ given $X_{t+1}$, so:

<d-math block>
\begin{aligned}
\hat{x}_{t|T} & = E[E[X_t|X_{t+1},Y_{1:t}]| Y_{1:T}] \\
 & = E[X_t|X_{t+1},Y_{1:t}] 
\end{aligned}
</d-math>

Using the formulas for the conditional Gaussian distribution:

## <d-math block>
##    \hat{x}_{t|T} = \hat{x}_{t|t} + L_t(\hat{x}_{t+1|T} -  \hat{x}_{t+1|t})
## </d-math>

## Similarly, 

## <d-math block>
##    P_{t|T} = Var[\hat_{t|T} | y_{1:T}] + E[Var[X_t|X_{t+1},y_{1:t}}]|y_{1:T}]
## </d-math>

## where, $L_t = P_{t\|t}A^TP^{-1}_{t+1\|t}$
